{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50c98960-94d0-4d20-bb30-b9fca37097c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IN this notebook we are going to load and finetune it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b5426fe-a276-4123-8252-f4f71da9dcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8cf7f9a-0a77-425c-a9a3-a5dff68aee44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset = load_dataset(\"glue\",\"mrpc\")\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6990cd2f-dcc4-4929-b1e5-ccdbbcba3965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e527b5cf803d4bf19603a01caf97ae77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  17%|#6        | 73.4M/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rajkr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\rajkr\\.cache\\huggingface\\hub\\models--bert-base-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"bert-base-cased\" #picking a model\n",
    "#loading the model and weights\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint,num_labels=2)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "#compile and add an optimizer\n",
    "model.compile(optimizer='adam',loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "984a48ec-7bba-4b57-a3bf-131d44c95128",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=raw_dataset['train']\n",
    "vali_dataset=raw_dataset['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d39f853-502d-4e54-8aba-66f71c658f5c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: <class 'datasets.arrow_dataset.Dataset'>, <class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#training the inputs\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m          \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvali_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m          \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\n\u001b[0;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_tf_utils.py:1229\u001b[0m, in \u001b[0;36mTFPreTrainedModel.fit\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1226\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(keras\u001b[38;5;241m.\u001b[39mModel\u001b[38;5;241m.\u001b[39mfit)\n\u001b[0;32m   1227\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1228\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m convert_batch_encoding(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 1229\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\data_adapter.py:1105\u001b[0m, in \u001b[0;36mselect_data_adapter\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m   1102\u001b[0m adapter_cls \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mcls\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m ALL_ADAPTER_CLS \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mcan_handle(x, y)]\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m adapter_cls:\n\u001b[0;32m   1104\u001b[0m     \u001b[38;5;66;03m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[39;00m\n\u001b[1;32m-> 1105\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1106\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to find data adapter that can handle input: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1107\u001b[0m             _type_name(x), _type_name(y)\n\u001b[0;32m   1108\u001b[0m         )\n\u001b[0;32m   1109\u001b[0m     )\n\u001b[0;32m   1110\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(adapter_cls) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1111\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1112\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData adapters should be mutually exclusive for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1113\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhandling inputs. Found multiple adapters \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m to handle \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1114\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(adapter_cls, _type_name(x), _type_name(y))\n\u001b[0;32m   1115\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to find data adapter that can handle input: <class 'datasets.arrow_dataset.Dataset'>, <class 'NoneType'>"
     ]
    }
   ],
   "source": [
    "#training the inputs\n",
    "model.fit(train_dataset,\n",
    "          validation_data=vali_dataset,\n",
    "          epochs=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b541dbb-8652-4689-a677-660ca2e218ac",
   "metadata": {},
   "source": [
    "# full code for finetuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c59da234-a065-4e92-9a0b-cc1f6926588a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9ca6cb2a8b747fa82498b1be887eba9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/408 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the dataset\n",
    "raw_dataset = load_dataset('glue', 'mrpc')\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')  # or your preferred model\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['sentence1'], \n",
    "        examples['sentence2'], \n",
    "        padding='max_length', \n",
    "        truncation=True,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "\n",
    "# Apply tokenization to train and validation datasets\n",
    "tokenized_datasets = raw_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Remove the original sentence columns and keep the tokenized inputs and labels\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['sentence1', 'sentence2', 'idx'])\n",
    "tokenized_datasets.set_format('tensorflow')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbed29de-7044-40d4-9e34-f406cc00fa24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rajkr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\arrow_dataset.py:401: FutureWarning: The output of `to_tf_dataset` will change when a passing single element list for `labels` or `columns` in the next datasets version. To return a tuple structure rather than dict, pass a single string.\n",
      "Old behaviour: columns=['a'], labels=['labels'] -> (tf.Tensor, tf.Tensor)  \n",
      "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor)  \n",
      "New behaviour: columns=['a'],labels=['labels'] -> ({'a': tf.Tensor}, {'labels': tf.Tensor})  \n",
      "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor) \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Convert to tensorflow dataset\n",
    "# Convert the tokenized dataset to TensorFlow Dataset\n",
    "train_dataset_tf = tokenized_datasets['train'].to_tf_dataset(\n",
    "    columns=['input_ids', 'attention_mask'],  # Model input features\n",
    "    label_cols=['label'],                     # Target column\n",
    "    shuffle=True,\n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "vali_dataset_tf = tokenized_datasets['validation'].to_tf_dataset(\n",
    "    columns=['input_ids', 'attention_mask'], \n",
    "    label_cols=['label'], \n",
    "    shuffle=False,\n",
    "    batch_size=8\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7868a21e-5440-49b7-9823-92295c4ab5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "  1/459 [..............................] - ETA: 13:10:59 - loss: 0.6836"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "# Assuming 'model' is your pre-trained model\n",
    "model.fit(train_dataset_tf, validation_data=vali_dataset_tf, epochs=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9f6a79-d8e0-4389-b0a4-917dad18ed02",
   "metadata": {},
   "source": [
    "# Learining rate scheduing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98232e0d-0220-4bd6-9cf9-1d223c38fb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will helps us to train or fine-tune in better way "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5c2e456-8d74-48e0-b319-242a99ce12e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#larning rate schedulingal\n",
    "from tensorflow.keras.optimizers.schedules import PolynomialDecay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32e337b-cbe2-4ecc-bb0a-2ee4f12dd36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "num_train_steps = len(train_dataset_tf) * num_epochs\n",
    "lr_schedular = PolynimialDecay(\n",
    "    initial_learning_rate = 5e-5,end_learning_rate=0.0,\n",
    "    decay_steps = num_train_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9247df7a-682d-4aac-9a62-6bc4f64eed51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# now for use this in efficient way for training we need to pass it with Adam\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673253d8-096c-403d-8550-c867a3ba4ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(learning_rate=lr_schedualr) #optimizer\n",
    "model.complie(loss=loss,optimizer=opt) #compiling with new learning rate schedualr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c83ad2-2994-4368-bf18-3db70eb56bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_dataset_tf,epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350e29c1-70f7-4145-9b80-05e9bbb96529",
   "metadata": {},
   "source": [
    "# Tensorlow Predictions and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5d1d89-f911-49d9-933a-9808d28e2e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(tokenized_datasets['validation'])['logits'] # passing tokenized text in this method for prediction\n",
    "probabilities = tf.nn.softmax(preds)\n",
    "class_preds = np.argmax(probabilities,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d178ec8f-ddaa-4cc1-94cd-2cf00b0ff784",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the GLUE Metrics\n",
    "from datasets import load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bbce5f28-50f3-4728-8c2d-706fabccecec",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_dataset(\"glue\",\"mrpc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423c11fd-e576-4146-b89f-8770d1c2d5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric.compute(predictions=class_preds,references=validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c411089e-1c91-49a0-9fdc-82e80b28c28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#native keras metrics way\n",
    "model.compile(loss=loss,opt=opt,metrics=['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a531bb71-3eb8-4e3b-8f80-6be26669717c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1568698a-4892-4827-92c6-ea71ae49963d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
