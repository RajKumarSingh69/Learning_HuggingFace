{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f264ffdd-fb6f-4172-b76f-73de260105d9",
   "metadata": {},
   "source": [
    "# we learn about preprocessing pairs of sentences in this notebook\n",
    "we will tokenize multiple sentences and do other stufs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce760a60-6510-4138-a325-8b441d9863a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "#the tokenizers can accept sentence in pairs as well as single sentence\n",
    "# The tokenizer adda special tokens for the corresponding model and prepares tokens types IDs\n",
    "# to indicate which part of the input correspond to which sentence\n",
    "#Tokenizers prepare the proper token type IDs and attention masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6885db5-68a6-4d52-a7f9-0f735cc20222",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "sequences = [\n",
    "    \"I've been waiting for a huggingface couse my whole life\",\n",
    "    \"This course is amazing\"]\n",
    "batch = tokenizer(sequences,padding=True,truncation=True,return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa8016bc-99e2-469c-a8a4-c37fec5704d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2522,  8557,  2026,  2878,  2166,   102],\n",
      "        [  101,  2023,  2607,  2003,  6429,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a66c13ca-56af-4535-8239-4de0239c80fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1045, 2572, 11948, 9600, 1012, 1045, 2052, 2066, 2000, 2022, 1037, 2204, 1998, 10514, 9623, 22747, 18083, 2951, 7155, 2030, 19875, 3992, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "checkpoint = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenizer(\"I am raj kumar. I would like to be a good and sucessfull data scientist or ML engineer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96dfe9e0-5af4-427f-9b71-9f9539ab3905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 2026, 2171, 2003, 11948, 9600, 5960, 102, 1045, 2215, 2000, 2468, 1037, 2951, 7155, 2030, 19875, 3992, 102, 0, 0, 0, 0, 0, 0, 0], [101, 1045, 2572, 2013, 29565, 1010, 16178, 2029, 2003, 2200, 2214, 2173, 1999, 2634, 102, 3728, 1045, 2288, 3479, 1999, 22975, 2015, 2004, 4619, 26758, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# passing multiple sentence with padding =True\n",
    "from transformers import AutoTokenizer\n",
    "checkpoint = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenizer(\n",
    "    ['My name is Raj Kumar Singh','I am from patna ,bihar which is very old place in india'],\n",
    "    ['i want to become a data scientist or ML Engineer',' Recently i got selected in TCS as Graduate trainee'],\n",
    "    padding=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3678d4-9e1d-4563-8dd0-7941d04672c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a8e6acc-e829-4c89-b23f-1e6bc6e6dd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff25372d-360e-493f-924b-dd02e8493d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "batch = tokenizer(\n",
    "    ['My name is Raj Kumar Singh','I am from patna ,bihar which is very old place in india'],\n",
    "    ['i want to become a data scientist or ML Engineer',' Recently i got selected in TCS as Graduate trainee'],\n",
    "    padding=True, return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2201f2b-07c6-4990-b700-6e9f4cf33da5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bece0ff6dd10406299a3b4c2f4ea7619",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rajkr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\rajkr\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Those inputs are then ready to go through a sqeuence classification models\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bce89d0f-c1aa-42a0-9d43-f6d98523dda0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.6206, -0.2568],\n",
       "        [ 0.6936, -0.2397]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e1edb6-8a74-4f87-bf2b-1f4fd746a851",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
