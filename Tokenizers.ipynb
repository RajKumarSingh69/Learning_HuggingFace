{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49e58044-bd4b-49ba-bc7d-8072a6bf0075",
   "metadata": {},
   "source": [
    "# Tokenizers in huggingface\n",
    "In this file we are going to learn about the tokenizers and its implemention in huggingface \n",
    "* There are 3 types of tokenizers\n",
    "* word-based tokeizer\n",
    "* character-based tokenizers\n",
    "* subword tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b82f22-7a3e-4aca-8fa4-a96269cacac4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8df8aa8a-7ad0-41a5-b63c-797afc29da80",
   "metadata": {},
   "source": [
    "# The Tokenizer Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24dc6a91-3c1f-4ed1-8a19-1ec0ca48da22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "640da23d-ea41-47d5-8660-c6126f9fa45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2292, 1005, 1055, 3046, 2000, 19204, 4697, 999, 102]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "inputs = tokenizer(\"Let's try to tokenize!\")\n",
    "print(inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71361fce-5d11-4392-b62e-70d7b39a7539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] let's try to tokenize! [SEP]\n"
     ]
    }
   ],
   "source": [
    "# conveting base form tokens ids to text\n",
    "print(tokenizer.decode(inputs['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d00294f-283f-4856-868b-fe43d4304b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['let', \"'\", 's', 'try', 'to', 'token', '##ize']\n"
     ]
    }
   ],
   "source": [
    "# see what's happening in tokenization pipeline\n",
    "#1st the text will split into tokens\n",
    "tokens = tokenizer.tokenize(\"let's try to tokenize\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d009f2f1-8a35-485f-afb6-55352a969acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2nd setp is to split into sepcial tokens\n",
    "# then it will generate a unique text id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2abf847-7570-42db-9591-787b74408d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁let', \"'\", 's', '▁try', '▁to', '▁to', 'ken', 'ize']\n"
     ]
    }
   ],
   "source": [
    "#Try another model tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"albert-base-v1\")\n",
    "tokens = tokenizer.tokenize(\"Let's try to tokenize\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c8deecd-722a-481d-bea7-cb707f7b5be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[408, 22, 18, 1131, 20, 20, 2853, 2952]\n"
     ]
    }
   ],
   "source": [
    "#convert this tokens into Unique ids\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e31db566-e6f0-43bb-a8e2-fdd0ac83ec52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a AlbertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [2, 408, 22, 18, 1131, 20, 20, 2853, 2952, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "#the special tokens \n",
    "final_inputs = tokenizer.prepare_for_model(input_ids)\n",
    "print(final_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc801b45-42a0-4a51-9e42-d651c4beeb8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like effects leading schools netherlands jrischen struggle shotly\n"
     ]
    }
   ],
   "source": [
    "# conveting base form tokens ids to text\n",
    "print(tokenizer.decode(inputs['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7126da94-4695-497c-ae73-595b8eb6c879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2292, 1005, 1055, 3046, 2000, 19204, 4697, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "tokenizer =  AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "inputs = tokenizer(\"Let's try to tokenize\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa03d020-24da-4bc2-aa07-55e0040e8a24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
