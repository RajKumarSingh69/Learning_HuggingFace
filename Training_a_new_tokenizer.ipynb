{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76c7a261-eeca-4195-8941-11d0caeaa7e4",
   "metadata": {},
   "source": [
    "# Tranining a new tokenizer\n",
    "* A tokenizer will not be suitable if it has been traned on a corpus that is not similar to the one you will use to train your model from scratch\n",
    "    * Dissimilarites can aries from:-\n",
    "        * New language\n",
    "        * New characters\n",
    "        * New domain\n",
    "        * New Style  \n",
    "* you may want to consider training a new tokenizer so that you have a tokenizer suitable for the training used to train a language model from scratch\n",
    "* To train a new tokenizer these are the necessay steps:-\n",
    "    * Gather a corpus of text\n",
    "    * choose tokenizer architecture\n",
    "    * train the tokenizer on the corpus\n",
    "    * save the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6bc7970-7f7c-4f10-8a81-b7df462f9ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "629be501-300d-4d20-bb84-9a1c6c60fd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_chekpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_chekpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e52c28d-2c6f-47cc-94da-b1bf67e70b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['here', 'is', 'a', 'sentence', 'adapted', 'to', 'our', 'token', '##izer']\n"
     ]
    }
   ],
   "source": [
    "text =\" here is a sentence adapted to our tokenizer\"\n",
    "print(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d0a9470-696a-4914-83bb-cf453fb8872e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['उ', '##स', '##स', 'ध', '##न', 'क', '##ी', 'स', '##ह', '##ा', '##य', '##त', '##ा', 'क', '##ी', 'आ', '##श', '##ा', 'क', '##र', '##त']\n"
     ]
    }
   ],
   "source": [
    "text=\"उससे धन की सहायता की आशा करत\"\n",
    "print(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6023f5c6-8c0e-4b11-8477-5f9be7f55cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can easily see that this tokenize is not good for hindi words \n",
    "#beuase it will seerate or break a simple workd into mant tokens\n",
    "# so that's why evenry tokenixer is used for different language and for different task\n",
    "# if we want to use tokenizer then we need to build a new tokenizer which will understand hindi words easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b2d478f-c9b8-45d3-add4-9c4889ca730f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'medical', 'vocabulary', 'is', 'divided', 'into', 'many', 'sub', '-', 'to', '##ke', ':', 'para', '##ce', '##tam', '##ol', ',', 'ph', '##ray', '##ng', '##ities']\n"
     ]
    }
   ],
   "source": [
    "text = \"the medical vocabulary is divided into many sub-toke: paracetamol, phrayngities\"\n",
    "print(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f4a54e-b764-4d14-a70b-90b28839c473",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0287ed66-9dda-40b2-bb50-b59bb74c6896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are two ways to build a new tokenizer\n",
    "#1st by using same arcitecutre of other tokenizer \n",
    "#2nd completely desining and building new tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5ecce70-75df-4ebe-b326-481a2060cd06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AutoTokenizer.train_new_from_iterator(\\n    text_iterator,\\n    vocab_size,\\n    new_special_tokens=None,\\n    special_tokens_map=None,\\n    **kwargs\\n)'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The transformer library provides a very easy to use method to train a tokenizer\n",
    "# using a known architecture on a new corpus\n",
    "'''AutoTokenizer.train_new_from_iterator(\n",
    "    text_iterator,\n",
    "    vocab_size,\n",
    "    new_special_tokens=None,\n",
    "    special_tokens_map=None,\n",
    "    **kwargs\n",
    ")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "304a3019-2f13-4bad-94a4-4e22a752f5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first step is to gather a training corpus\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02bcd491-38a2-4f98-a977-6919e46a81f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rajkr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\load.py:1454: FutureWarning: The repository for code_search_net contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/code_search_net\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db0da2f927304488b0cfe245b2d585f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/8.44k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b14abce1ee0439e8ee6ac0901663c84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/12.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00954d1642ca4b39a8d8d96f2c29df10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/941M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9b7392e2e3944aebbac213710ebcc44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/412178 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed0c8ca72cef4c9a97f199912718da33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/22176 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "641a682459ec4bdab2a7d12ffb870a9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/23107 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_dataset =  load_dataset(\"code_search_net\",\"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11db8d4a-1b24-4d08-8368-561943cfd062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after that\n",
    "def get_training_corpus():\n",
    "    dataset = raw_dataset['train']\n",
    "    for start_idx in range(0,len(dataset),100):\n",
    "        samples = dataset[start_idx : start_idx+1000]\n",
    "        yield samples[\"whole_func_string\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c7f13f4-aff1-4250-a339-956b020c2d18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d17b0e7027e421fa66c9eeb0ec12ef8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rajkr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\rajkr\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\rajkr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f590f0939c6c45e19a8105a45c85ec60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0474ba3d349f48b09c30aa9fc2fbe8b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "090d9ee9fb89463d8e0b1dcbb60ddbca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb736c22ed2a4297ab5f9f0cc62c8b2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_corpus = get_training_corpus()\n",
    "#loading gpt2 tokenizer because we are going to use it's architecture for building our new tokenizer\n",
    "old_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abe10d6-12b2-4236-b047-107d8b898ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokenizer = old_tokenizer.train_new_from_iterator(training_corpus,52000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f4dee0-0100-4404-b719-09f81e702126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can save our new tokenizer\n",
    "new_tokenizer.save_pretrained(\"code-search-net-tokenizer\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
